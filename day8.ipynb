{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1uHEa14jVS46SO07BkSEM4NwwavC4PmVe","authorship_tag":"ABX9TyNNRnYVmpCntSQBqug0fLxp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"spMf8Rnt2-VN","executionInfo":{"status":"ok","timestamp":1716978031210,"user_tz":-540,"elapsed":338,"user":{"displayName":"허진경 (나자바바)","userId":"08806215878196109838"}}},"outputs":[],"source":["from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, concatenate"]},{"cell_type":"code","source":["model_input = Input(shape=(28,28,3))\n","pre = Dense(192)(model_input)\n","\n","conv1 = Conv2D(64, 1, padding='same')(pre)\n","\n","conv1_2 = Conv2D(96, 1, padding='same')(pre)  # 1x1\n","conv2 = Conv2D(128, 3, padding='same')(conv1_2)\n","\n","conv1_3 = Conv2D(16, 1, padding='same')(pre) # 1x1\n","conv3 = Conv2D(32, 5, padding='same')(conv1_3)\n","\n","pool = MaxPooling2D(pool_size=(3,3), strides=1, padding='same')(pre)\n","conv1_4 = Conv2D(32, 1, padding='same')(pool) # 1x1\n","\n","model_output = concatenate([conv1, conv2, conv3, conv1_4])\n","model = Model(inputs=model_input, outputs=model_output)\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D9oWQtxh3TXw","executionInfo":{"status":"ok","timestamp":1716978466594,"user_tz":-540,"elapsed":552,"user":{"displayName":"허진경 (나자바바)","userId":"08806215878196109838"}},"outputId":"4d082abe-3325-407a-d7c5-0dc3b2f74ec3"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_3 (InputLayer)        [(None, 28, 28, 3)]          0         []                            \n","                                                                                                  \n"," dense_2 (Dense)             (None, 28, 28, 192)          768       ['input_3[0][0]']             \n","                                                                                                  \n"," conv2d_6 (Conv2D)           (None, 28, 28, 96)           18528     ['dense_2[0][0]']             \n","                                                                                                  \n"," conv2d_8 (Conv2D)           (None, 28, 28, 16)           3088      ['dense_2[0][0]']             \n","                                                                                                  \n"," max_pooling2d (MaxPooling2  (None, 28, 28, 192)          0         ['dense_2[0][0]']             \n"," D)                                                                                               \n","                                                                                                  \n"," conv2d_5 (Conv2D)           (None, 28, 28, 64)           12352     ['dense_2[0][0]']             \n","                                                                                                  \n"," conv2d_7 (Conv2D)           (None, 28, 28, 128)          110720    ['conv2d_6[0][0]']            \n","                                                                                                  \n"," conv2d_9 (Conv2D)           (None, 28, 28, 32)           12832     ['conv2d_8[0][0]']            \n","                                                                                                  \n"," conv2d_10 (Conv2D)          (None, 28, 28, 32)           6176      ['max_pooling2d[0][0]']       \n","                                                                                                  \n"," concatenate (Concatenate)   (None, 28, 28, 256)          0         ['conv2d_5[0][0]',            \n","                                                                     'conv2d_7[0][0]',            \n","                                                                     'conv2d_9[0][0]',            \n","                                                                     'conv2d_10[0][0]']           \n","                                                                                                  \n","==================================================================================================\n","Total params: 164464 (642.44 KB)\n","Trainable params: 164464 (642.44 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Conv2D, Activation, add"],"metadata":{"id":"_LaFWXOZ43m8","executionInfo":{"status":"ok","timestamp":1716979525356,"user_tz":-540,"elapsed":312,"user":{"displayName":"허진경 (나자바바)","userId":"08806215878196109838"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["model_input = Input(shape=(28,28,3))\n","\n","x = Conv2D(128, 3, padding='same', activation='relu')(model_input)\n","\n","conv = Conv2D(64, 3, padding='same', activation='relu')(x)\n","conv = Conv2D(128, 3, padding='same')(conv)\n","\n","y = add([conv, x])\n","y = Activation('relu')(y)\n","\n","model = Model(inputs=model_input, outputs=y)\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ra6kkbwe9GLh","executionInfo":{"status":"ok","timestamp":1716979718381,"user_tz":-540,"elapsed":305,"user":{"displayName":"허진경 (나자바바)","userId":"08806215878196109838"}},"outputId":"f880825a-2b35-49c2-8982-e7dde5ed0b4e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_4 (InputLayer)        [(None, 28, 28, 3)]          0         []                            \n","                                                                                                  \n"," conv2d_11 (Conv2D)          (None, 28, 28, 128)          3584      ['input_4[0][0]']             \n","                                                                                                  \n"," conv2d_12 (Conv2D)          (None, 28, 28, 64)           73792     ['conv2d_11[0][0]']           \n","                                                                                                  \n"," conv2d_13 (Conv2D)          (None, 28, 28, 128)          73856     ['conv2d_12[0][0]']           \n","                                                                                                  \n"," add (Add)                   (None, 28, 28, 128)          0         ['conv2d_13[0][0]',           \n","                                                                     'conv2d_11[0][0]']           \n","                                                                                                  \n"," activation (Activation)     (None, 28, 28, 128)          0         ['add[0][0]']                 \n","                                                                                                  \n","==================================================================================================\n","Total params: 151232 (590.75 KB)\n","Trainable params: 151232 (590.75 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["# CIFAR10 이미지 분류 모델을 Residual Block을 이용해서 구현\n","\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import *\n","\n","inputs = Input(shape=(32,32,3))\n","\n","conv = Conv2D(32, 3, activation='relu', padding='same')(inputs)\n","# x = conv(inputs)\n","x = BatchNormalization()(conv)\n","\n","# Residual Block을 2개\n","# 1번째 잔차블록\n","conv = Conv2D(32, 3, activation='relu', padding='same')(x)\n","y = BatchNormalization()(conv)\n","\n","conv = Conv2D(32, 3, padding='same')(y)\n","y = BatchNormalization()(conv)\n","\n","x = add([x, y])\n","x = Activation('relu')(x)\n","\n","# 2번째 잔차블록\n","conv = Conv2D(32, 3, activation='relu', padding='same')(x)\n","y = BatchNormalization()(conv)\n","\n","conv = Conv2D(32, 3, padding='same')(y)\n","y = BatchNormalization()(conv)\n","\n","x = add([x, y])\n","x = Activation('relu')(x)\n","\n","x = MaxPooling2D()(x)\n","y = Flatten()(x)\n","\n","y = Dense(512, activation='relu')(y)\n","outputs = Dense(10, activation='softmax')(y)\n","\n","model = Model(inputs=inputs, outputs=outputs)\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"EyOywKI3DBwQ","executionInfo":{"status":"ok","timestamp":1716982230371,"user_tz":-540,"elapsed":791,"user":{"displayName":"허진경 (나자바바)","userId":"08806215878196109838"}},"outputId":"270597a4-1ccf-4c4f-9e0e-0b5829d05151"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_2\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_5 (InputLayer)        [(None, 32, 32, 3)]          0         []                            \n","                                                                                                  \n"," conv2d_14 (Conv2D)          (None, 32, 32, 32)           896       ['input_5[0][0]']             \n","                                                                                                  \n"," batch_normalization (Batch  (None, 32, 32, 32)           128       ['conv2d_14[0][0]']           \n"," Normalization)                                                                                   \n","                                                                                                  \n"," conv2d_15 (Conv2D)          (None, 32, 32, 32)           9248      ['batch_normalization[0][0]'] \n","                                                                                                  \n"," batch_normalization_1 (Bat  (None, 32, 32, 32)           128       ['conv2d_15[0][0]']           \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," conv2d_16 (Conv2D)          (None, 32, 32, 32)           9248      ['batch_normalization_1[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," batch_normalization_2 (Bat  (None, 32, 32, 32)           128       ['conv2d_16[0][0]']           \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," add_1 (Add)                 (None, 32, 32, 32)           0         ['batch_normalization[0][0]', \n","                                                                     'batch_normalization_2[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," activation_1 (Activation)   (None, 32, 32, 32)           0         ['add_1[0][0]']               \n","                                                                                                  \n"," conv2d_17 (Conv2D)          (None, 32, 32, 32)           9248      ['activation_1[0][0]']        \n","                                                                                                  \n"," batch_normalization_3 (Bat  (None, 32, 32, 32)           128       ['conv2d_17[0][0]']           \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," conv2d_18 (Conv2D)          (None, 32, 32, 32)           9248      ['batch_normalization_3[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," batch_normalization_4 (Bat  (None, 32, 32, 32)           128       ['conv2d_18[0][0]']           \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," add_2 (Add)                 (None, 32, 32, 32)           0         ['activation_1[0][0]',        \n","                                                                     'batch_normalization_4[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," activation_2 (Activation)   (None, 32, 32, 32)           0         ['add_2[0][0]']               \n","                                                                                                  \n"," max_pooling2d_1 (MaxPoolin  (None, 16, 16, 32)           0         ['activation_2[0][0]']        \n"," g2D)                                                                                             \n","                                                                                                  \n"," flatten (Flatten)           (None, 8192)                 0         ['max_pooling2d_1[0][0]']     \n","                                                                                                  \n"," dense_3 (Dense)             (None, 512)                  4194816   ['flatten[0][0]']             \n","                                                                                                  \n"," dense_4 (Dense)             (None, 10)                   5130      ['dense_3[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 4238474 (16.17 MB)\n","Trainable params: 4238154 (16.17 MB)\n","Non-trainable params: 320 (1.25 KB)\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.datasets import cifar10\n","(train_X, train_y), (test_X, test_y) = cifar10.load_data()\n","train_X = train_X / 255.0\n","test_X = test_X / 255.0\n","print(train_X.shape, test_X.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5U034E9MHuDR","executionInfo":{"status":"ok","timestamp":1716982899485,"user_tz":-540,"elapsed":5052,"user":{"displayName":"허진경 (나자바바)","userId":"08806215878196109838"}},"outputId":"7059f788-4828-4109-c334-952420c50a8e"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170498071/170498071 [==============================] - 2s 0us/step\n","(50000, 32, 32, 3) (10000, 32, 32, 3)\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.callbacks import TensorBoard\n","tensorboard = TensorBoard(log_dir='logs', histogram_freq=1, embeddings_freq=1)\n","\n","model.compile(loss='sparse_categorical_crossentropy',\n","              optimizer='adam', metrics=['accuracy'])\n","\n","model.fit(train_X, train_y, validation_data=(test_X, test_y),\n","          epochs=30, batch_size=200,\n","          callbacks=[tensorboard])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"_nr173gPJ82U","executionInfo":{"status":"ok","timestamp":1716983559961,"user_tz":-540,"elapsed":311413,"user":{"displayName":"허진경 (나자바바)","userId":"08806215878196109838"}},"outputId":"f9580e35-e0dc-4d7c-efda-da4f1842a5c7"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","250/250 [==============================] - 17s 44ms/step - loss: 2.8891 - accuracy: 0.3536 - val_loss: 1.9299 - val_accuracy: 0.2880\n","Epoch 2/30\n","250/250 [==============================] - 10s 39ms/step - loss: 1.3028 - accuracy: 0.5343 - val_loss: 1.4620 - val_accuracy: 0.4821\n","Epoch 3/30\n","250/250 [==============================] - 10s 40ms/step - loss: 1.0821 - accuracy: 0.6150 - val_loss: 1.1814 - val_accuracy: 0.5855\n","Epoch 4/30\n","250/250 [==============================] - 10s 40ms/step - loss: 0.9200 - accuracy: 0.6764 - val_loss: 1.0354 - val_accuracy: 0.6384\n","Epoch 5/30\n","250/250 [==============================] - 10s 39ms/step - loss: 0.8261 - accuracy: 0.7116 - val_loss: 1.0023 - val_accuracy: 0.6515\n","Epoch 6/30\n","250/250 [==============================] - 11s 42ms/step - loss: 0.7498 - accuracy: 0.7370 - val_loss: 0.9242 - val_accuracy: 0.6827\n","Epoch 7/30\n","250/250 [==============================] - 10s 41ms/step - loss: 0.6880 - accuracy: 0.7589 - val_loss: 0.9407 - val_accuracy: 0.6763\n","Epoch 8/30\n","250/250 [==============================] - 10s 39ms/step - loss: 0.6335 - accuracy: 0.7769 - val_loss: 0.9186 - val_accuracy: 0.6910\n","Epoch 9/30\n","250/250 [==============================] - 10s 40ms/step - loss: 0.5864 - accuracy: 0.7942 - val_loss: 0.9829 - val_accuracy: 0.6822\n","Epoch 10/30\n","250/250 [==============================] - 10s 40ms/step - loss: 0.5299 - accuracy: 0.8136 - val_loss: 0.9305 - val_accuracy: 0.6967\n","Epoch 11/30\n","250/250 [==============================] - 11s 43ms/step - loss: 0.4950 - accuracy: 0.8274 - val_loss: 0.9147 - val_accuracy: 0.7001\n","Epoch 12/30\n","250/250 [==============================] - 10s 40ms/step - loss: 0.4515 - accuracy: 0.8416 - val_loss: 1.1528 - val_accuracy: 0.6435\n","Epoch 13/30\n","250/250 [==============================] - 10s 39ms/step - loss: 0.4208 - accuracy: 0.8519 - val_loss: 1.0232 - val_accuracy: 0.6950\n","Epoch 14/30\n","250/250 [==============================] - 10s 39ms/step - loss: 0.3667 - accuracy: 0.8709 - val_loss: 1.0241 - val_accuracy: 0.6969\n","Epoch 15/30\n","250/250 [==============================] - 10s 39ms/step - loss: 0.3404 - accuracy: 0.8792 - val_loss: 1.1648 - val_accuracy: 0.6917\n","Epoch 16/30\n","250/250 [==============================] - 10s 42ms/step - loss: 0.2971 - accuracy: 0.8955 - val_loss: 1.0908 - val_accuracy: 0.7012\n","Epoch 17/30\n","250/250 [==============================] - 10s 40ms/step - loss: 0.2587 - accuracy: 0.9085 - val_loss: 1.1434 - val_accuracy: 0.6998\n","Epoch 18/30\n","250/250 [==============================] - 10s 40ms/step - loss: 0.2248 - accuracy: 0.9223 - val_loss: 1.2299 - val_accuracy: 0.6944\n","Epoch 19/30\n","250/250 [==============================] - 10s 39ms/step - loss: 0.2171 - accuracy: 0.9235 - val_loss: 1.3861 - val_accuracy: 0.6750\n","Epoch 20/30\n","250/250 [==============================] - 10s 40ms/step - loss: 0.1823 - accuracy: 0.9357 - val_loss: 1.4153 - val_accuracy: 0.6876\n","Epoch 21/30\n","250/250 [==============================] - 10s 40ms/step - loss: 0.1721 - accuracy: 0.9392 - val_loss: 1.4421 - val_accuracy: 0.6882\n","Epoch 22/30\n","250/250 [==============================] - 10s 41ms/step - loss: 0.1472 - accuracy: 0.9481 - val_loss: 1.5166 - val_accuracy: 0.6919\n","Epoch 23/30\n","250/250 [==============================] - 10s 40ms/step - loss: 0.1412 - accuracy: 0.9515 - val_loss: 1.5989 - val_accuracy: 0.6872\n","Epoch 24/30\n","250/250 [==============================] - 10s 40ms/step - loss: 0.1275 - accuracy: 0.9550 - val_loss: 1.5596 - val_accuracy: 0.6985\n","Epoch 25/30\n","250/250 [==============================] - 10s 39ms/step - loss: 0.0857 - accuracy: 0.9716 - val_loss: 1.7177 - val_accuracy: 0.6835\n","Epoch 26/30\n","250/250 [==============================] - 10s 40ms/step - loss: 0.0805 - accuracy: 0.9737 - val_loss: 1.6831 - val_accuracy: 0.7002\n","Epoch 27/30\n","250/250 [==============================] - 10s 41ms/step - loss: 0.0891 - accuracy: 0.9698 - val_loss: 1.7758 - val_accuracy: 0.6917\n","Epoch 28/30\n","250/250 [==============================] - 10s 40ms/step - loss: 0.1097 - accuracy: 0.9615 - val_loss: 1.7406 - val_accuracy: 0.6827\n","Epoch 29/30\n","250/250 [==============================] - 10s 39ms/step - loss: 0.1015 - accuracy: 0.9639 - val_loss: 1.7662 - val_accuracy: 0.7045\n","Epoch 30/30\n","250/250 [==============================] - 10s 39ms/step - loss: 0.0753 - accuracy: 0.9744 - val_loss: 1.8840 - val_accuracy: 0.6897\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7b9c2b372cb0>"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["from tensorflow.keras.applications import ResNet50\n","model = ResNet50()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iyEeJpzkRSjA","executionInfo":{"status":"ok","timestamp":1716984892466,"user_tz":-540,"elapsed":5262,"user":{"displayName":"허진경 (나자바바)","userId":"08806215878196109838"}},"outputId":"a356b467-436b-407d-f614-e05a5d9648b1"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n","102967424/102967424 [==============================] - 1s 0us/step\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing import image\n","# 이미지 라이브러리: OpenCV, PIL\n","img = image.load_img(\"YellowLabradorLooking_new.jpg\", target_size=(224,224))\n","x = image.img_to_array(img).reshape(-1, 224, 224, 3)\n","pred = model.predict(x)\n","print(pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"heVro4BrRjWw","executionInfo":{"status":"ok","timestamp":1716985175521,"user_tz":-540,"elapsed":2766,"user":{"displayName":"허진경 (나자바바)","userId":"08806215878196109838"}},"outputId":"e921f8c3-96ac-41d7-99e5-d41e5395e00f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 2s 2s/step\n","[[4.33807872e-06 1.52540497e-06 6.96436155e-06 1.65774927e-05\n","  1.08696186e-05 1.06768039e-06 6.00921965e-07 1.21360699e-05\n","  4.55568579e-06 3.19667379e-06 6.29502281e-08 6.10987342e-07\n","  6.12321912e-07 3.46508045e-07 1.45254512e-06 1.84455487e-07\n","  8.59957197e-07 2.49889399e-06 5.77762137e-07 3.60813061e-07\n","  1.13777332e-07 2.26864313e-06 8.09308574e-07 2.68382701e-05\n","  6.21508389e-06 1.01057401e-06 4.53236879e-07 7.03097271e-07\n","  8.61081503e-07 7.07881918e-06 1.55072894e-06 3.08694689e-06\n","  9.61662636e-07 9.06477271e-06 5.07999084e-06 2.02075299e-07\n","  1.43051659e-06 5.39846087e-06 1.11721545e-06 2.96722442e-06\n","  1.11545239e-06 1.05331310e-06 1.69657540e-06 1.68659119e-06\n","  1.36466610e-06 2.53124790e-06 3.83963925e-06 3.62667879e-06\n","  1.89725233e-05 5.31404794e-07 2.28100043e-06 2.60282127e-06\n","  8.52118774e-06 6.79506627e-07 7.75838942e-07 5.58854481e-06\n","  7.05373338e-07 4.55737364e-07 1.12284783e-06 2.12057762e-06\n","  1.00536681e-06 8.00528960e-06 1.42915303e-06 9.21466778e-07\n","  2.55933719e-06 4.68701774e-06 7.13381837e-07 6.83582584e-07\n","  4.48321089e-07 2.98325290e-06 9.08682125e-07 4.26772476e-06\n","  2.70935698e-06 4.12421787e-06 9.30386705e-06 1.60395700e-06\n","  2.52786322e-06 9.12172163e-06 6.13828388e-06 2.00013837e-05\n","  1.22681328e-07 4.72596582e-07 5.94213361e-07 3.48130413e-07\n","  1.52250759e-05 2.18869332e-06 8.44432350e-07 7.92498395e-06\n","  1.35729260e-06 8.05745640e-06 3.13773080e-06 1.45844368e-07\n","  5.18680622e-07 9.09380049e-07 9.14908185e-07 1.43096418e-06\n","  1.99832698e-07 9.40222833e-07 6.48362217e-08 1.36780750e-06\n","  7.45844181e-06 1.19572760e-05 3.19511332e-06 4.50653459e-07\n","  9.79668403e-06 1.28306033e-06 9.19759145e-07 9.75022067e-06\n","  1.98803104e-06 5.66369465e-07 1.55299546e-07 4.28822977e-06\n","  5.23311610e-06 4.59252442e-06 6.69706969e-06 1.01670621e-05\n","  3.04300187e-07 4.87002853e-06 2.89834793e-06 1.83118937e-06\n","  1.28989341e-05 2.95695457e-07 1.19218578e-06 2.82878955e-06\n","  1.56327660e-05 1.00052421e-06 1.24319995e-05 1.51050313e-06\n","  1.10031479e-07 9.95831670e-06 6.36162076e-06 1.21778476e-06\n","  1.53137660e-06 9.02008566e-08 1.82755537e-06 1.01110902e-06\n","  1.23869268e-06 2.62813592e-06 1.98678208e-06 7.30005354e-07\n","  1.54056750e-07 7.63019585e-08 2.13336989e-07 6.52811195e-07\n","  5.21902621e-06 1.08994050e-06 5.24556253e-06 1.27285807e-06\n","  2.43179556e-07 7.51534071e-06 1.50099243e-06 5.72440797e-04\n","  4.46716840e-05 7.06397259e-05 1.37529923e-05 9.63760613e-05\n","  3.26748341e-05 1.93384767e-05 1.52327004e-04 7.02451216e-03\n","  3.01507564e-04 6.70373774e-05 7.27412896e-03 3.89416772e-03\n","  1.26464861e-02 3.46976332e-03 6.76496550e-02 2.03624200e-02\n","  1.60101671e-02 6.36001641e-04 1.42855349e-03 5.38347158e-05\n","  1.67071383e-04 5.18096099e-03 3.21421330e-03 3.46197252e-04\n","  2.92114820e-03 2.24487667e-04 2.55007930e-02 1.83330127e-03\n","  5.63716469e-03 1.10232574e-03 2.29731039e-03 1.20335619e-03\n","  3.09058203e-04 1.10339536e-03 9.93646881e-06 7.28874293e-05\n","  2.48837343e-04 3.84461746e-04 5.18974084e-05 1.24076952e-03\n","  2.05789693e-04 1.16817733e-04 3.21587177e-05 7.07382220e-04\n","  2.22984375e-03 1.86502859e-02 1.62828772e-03 6.05431233e-05\n","  1.79613847e-03 1.05501723e-03 8.73160607e-04 1.78501668e-05\n","  2.85370159e-04 1.42117834e-03 2.73771491e-03 5.85941225e-03\n","  2.68977433e-01 1.07565373e-01 4.57715336e-03 7.00721284e-04\n","  4.44907183e-03 7.29882857e-04 6.39658945e-04 1.60926906e-03\n","  2.32801191e-04 9.16706558e-05 6.98851318e-06 1.19916556e-04\n","  9.30784809e-05 6.41187362e-05 4.68686633e-02 4.52871202e-04\n","  7.62725394e-05 2.55091838e-03 3.49472284e-05 3.12869670e-03\n","  8.44337337e-05 1.68602433e-04 3.81892023e-05 4.39197611e-05\n","  2.15289372e-04 1.75547902e-04 9.52873658e-03 2.08771019e-03\n","  2.25093067e-02 4.48180508e-04 8.14372953e-03 3.15894635e-04\n","  5.17497596e-04 2.87583127e-04 1.51495442e-01 2.47236192e-02\n","  4.84912330e-03 1.82062504e-04 1.68553572e-02 4.28616721e-03\n","  2.97558145e-03 3.11829615e-04 2.14340072e-03 1.63909011e-02\n","  1.07220680e-04 1.13829935e-03 1.34081598e-02 1.81839423e-04\n","  1.32730976e-03 1.27502496e-03 1.09773227e-05 1.18513435e-05\n","  1.66559752e-04 2.29955804e-05 1.31764822e-03 4.43836427e-07\n","  1.50765061e-06 1.52186680e-04 1.93224667e-04 6.47294102e-03\n","  8.02745635e-05 4.93200787e-05 2.38447174e-04 4.20534634e-05\n","  7.89418846e-05 2.49902764e-03 1.00218349e-05 1.49703619e-05\n","  1.89773266e-06 1.79851739e-07 5.33145453e-07 2.67874725e-06\n","  9.42069050e-07 1.02776521e-05 1.62199212e-05 5.71546082e-08\n","  1.01347196e-05 1.22809533e-05 2.19923459e-06 1.06652942e-06\n","  1.97277836e-06 1.18564924e-06 2.56127350e-06 4.10875982e-05\n","  1.45673257e-05 5.15168949e-06 2.11322322e-06 4.23169325e-07\n","  3.61363527e-06 4.50878701e-07 9.12917471e-07 2.18329078e-05\n","  1.22783581e-06 8.32608930e-06 1.15309524e-06 3.52829329e-06\n","  1.34951806e-05 2.76222522e-06 1.19015294e-05 8.38340839e-06\n","  2.22029780e-06 4.86544741e-06 5.82977918e-06 1.08980850e-04\n","  5.50812365e-06 2.62673757e-06 1.44064161e-05 2.16054696e-06\n","  2.66677489e-06 1.76098147e-05 7.04814511e-07 2.97535416e-06\n","  4.39082839e-07 8.98949224e-07 4.25221970e-07 1.14323393e-06\n","  8.43932355e-07 1.41655320e-07 1.18264768e-06 3.84497289e-05\n","  6.07715356e-06 2.27585929e-06 2.07445942e-06 2.07838148e-05\n","  3.14196313e-07 4.33803109e-07 8.91177251e-07 1.29511477e-07\n","  5.59314550e-08 4.35927291e-07 1.18445917e-06 1.45154409e-04\n","  1.97480094e-05 7.09502592e-06 6.72303997e-07 6.21401114e-06\n","  1.70703856e-06 2.65757026e-05 3.02221497e-06 1.39638621e-06\n","  1.50967506e-04 1.06229390e-05 8.08621962e-07 5.07657887e-07\n","  4.59405044e-07 6.07471975e-06 5.08182529e-05 7.09500591e-06\n","  1.72223611e-06 3.99707324e-06 3.87062528e-06 1.98250837e-06\n","  7.09875849e-07 1.52756377e-06 8.14040504e-07 6.02102273e-06\n","  1.66805171e-06 5.82247822e-06 6.57261808e-06 1.07047083e-06\n","  5.74953674e-06 5.24427492e-07 8.39855318e-07 2.37842833e-05\n","  7.48374987e-06 1.50196047e-06 5.90754780e-07 1.04635581e-06\n","  3.66252380e-06 1.96648193e-06 1.15025284e-06 2.08243978e-06\n","  4.04591765e-06 1.03973491e-06 9.53493952e-07 1.15803232e-06\n","  5.25531391e-07 6.95362360e-06 5.29713088e-06 4.48397202e-07\n","  7.14391490e-07 1.33273788e-05 2.56136150e-06 6.37091625e-06\n","  6.63851381e-07 4.37501421e-06 3.83916886e-05 1.24993576e-05\n","  1.27533394e-05 2.24152222e-06 2.82982819e-06 5.60237049e-06\n","  2.18041951e-05 8.81747383e-06 1.99836609e-06 1.88680224e-06\n","  5.36911500e-07 1.65946460e-06 1.74535955e-06 8.99658062e-06\n","  3.47855803e-06 6.46381704e-06 2.53572011e-06 9.42404768e-06\n","  2.27952041e-05 2.92868094e-06 7.47289505e-06 2.34691043e-07\n","  6.95347762e-06 1.46558805e-05 4.79338596e-05 9.52800019e-06\n","  9.21580704e-06 6.64642903e-06 1.49498746e-05 1.00747702e-05\n","  2.73098431e-06 2.22321546e-06 6.25332120e-07 1.46718930e-05\n","  1.08602515e-04 7.15156693e-06 5.29916178e-06 8.48000927e-05\n","  1.68582501e-05 9.69004032e-06 1.91882043e-03 5.54471035e-06\n","  4.86054478e-06 6.34859634e-06 2.92970594e-06 8.04291631e-06\n","  1.84704932e-05 7.83191535e-06 3.37781944e-06 2.61230707e-05\n","  8.23663231e-06 8.53449747e-05 9.59403678e-07 3.63624076e-06\n","  4.35696029e-06 2.57185229e-06 3.54618192e-06 3.07801492e-05\n","  7.10085806e-05 2.46669038e-06 5.44921886e-06 2.46402669e-06\n","  1.33389367e-05 5.97778890e-05 2.85604165e-05 1.21184903e-05\n","  3.31099250e-06 4.83015356e-06 1.38715222e-06 6.92614703e-05\n","  1.18675962e-05 2.69588872e-05 6.19911589e-07 2.47817115e-06\n","  2.45220363e-05 3.77330580e-05 4.77873618e-06 2.91804213e-06\n","  1.03925959e-04 7.27788256e-06 1.17629008e-06 5.90973150e-06\n","  4.61489799e-06 1.24563326e-06 5.28417377e-06 1.58232269e-05\n","  4.24427490e-05 1.46095852e-06 5.40625069e-06 8.66171831e-06\n","  8.25293955e-06 1.35395453e-06 2.03503805e-06 1.57664235e-05\n","  3.50274263e-06 1.15780658e-05 6.94799610e-06 1.59394460e-06\n","  1.01208407e-05 3.52633197e-06 8.85849147e-07 1.11366844e-05\n","  2.07460689e-05 1.38396240e-06 2.64216374e-06 6.28005682e-06\n","  1.22644667e-06 8.31185243e-05 4.38176176e-06 7.58173519e-06\n","  1.91075451e-05 1.77775073e-05 3.58412217e-06 1.40097945e-05\n","  1.84715664e-05 4.16826850e-07 2.34656932e-06 6.83296184e-06\n","  2.82490619e-05 2.41984471e-05 4.48461606e-06 6.35828837e-05\n","  1.57021568e-05 6.82983796e-07 7.83206451e-06 2.11157621e-05\n","  1.43418561e-06 2.66281018e-06 3.95399438e-05 1.51367421e-04\n","  9.12985888e-06 9.22019694e-07 3.38554014e-06 3.70209364e-06\n","  2.70410123e-06 3.90124878e-05 7.20049820e-06 5.80908954e-06\n","  6.52306790e-06 5.67688676e-06 8.18347326e-05 1.33896685e-06\n","  1.45954084e-06 1.76220128e-04 4.10912253e-06 3.08512506e-04\n","  9.56173835e-07 4.09187669e-06 3.70213593e-06 1.03872204e-04\n","  1.46314278e-05 2.83397912e-06 2.74781132e-06 2.81928095e-07\n","  1.01996977e-06 1.11417403e-05 1.96108272e-06 2.39182236e-06\n","  2.29253583e-05 6.97906216e-06 1.30913520e-06 1.61879859e-06\n","  1.37000211e-06 2.31201520e-05 7.70672500e-07 3.16390469e-05\n","  1.93249551e-04 5.15814463e-06 1.09358363e-04 1.09076916e-06\n","  1.43649083e-06 5.89168735e-07 3.95957341e-06 1.80256993e-05\n","  1.19352080e-05 3.25916631e-06 1.24248518e-05 5.32369313e-06\n","  1.17992986e-05 3.54926101e-06 1.98015496e-05 1.29028467e-05\n","  4.19390972e-06 2.79864480e-06 1.21130775e-04 1.86337584e-05\n","  1.55743533e-06 4.81827465e-06 5.00512169e-06 2.37645668e-06\n","  1.07094520e-05 1.97542449e-05 1.55587202e-06 7.23261792e-06\n","  3.71723036e-05 9.06086207e-05 1.49330663e-06 1.53013061e-05\n","  7.56291229e-07 5.14353042e-06 3.24259213e-06 1.82618771e-06\n","  6.96904244e-05 1.19698370e-05 1.93160713e-06 5.93672758e-06\n","  7.95539381e-06 8.20655532e-06 8.94987443e-06 2.21628161e-05\n","  1.14095656e-06 1.71113716e-05 3.51223252e-06 4.55009740e-06\n","  3.67018656e-04 3.44113650e-05 2.79605574e-05 5.88548482e-06\n","  3.03403181e-06 8.15635485e-06 1.02952317e-05 7.42164502e-06\n","  4.88763799e-05 1.95992652e-05 6.52125755e-06 3.17455692e-06\n","  3.24527900e-05 5.50957120e-05 8.33019476e-06 9.75925377e-06\n","  1.38701068e-06 1.34223728e-06 1.07017540e-05 1.72512591e-06\n","  1.52855466e-06 7.95231017e-07 4.94052574e-06 7.74872115e-06\n","  1.16753190e-05 4.76683363e-06 7.63964351e-07 2.93690937e-06\n","  5.83328256e-06 3.50906375e-05 1.38060888e-04 7.20502139e-05\n","  3.45856336e-07 4.36737064e-05 4.81297457e-06 1.08713812e-05\n","  7.48171296e-05 5.93535742e-06 1.10153178e-05 9.04989065e-06\n","  2.47736762e-06 9.77311629e-07 8.13102906e-06 3.63398703e-06\n","  1.49111729e-05 2.09031605e-05 5.86065016e-06 2.46788713e-05\n","  1.41645060e-05 4.48770015e-06 3.86756619e-06 3.10592873e-06\n","  2.39071642e-06 4.27279065e-06 8.22733000e-06 9.20349635e-07\n","  1.40622269e-05 2.93857374e-06 2.32382058e-06 2.64947485e-05\n","  1.78505661e-06 4.85268320e-05 2.75679849e-06 2.80754648e-06\n","  1.78708797e-05 5.64689253e-06 2.03280451e-05 2.42790838e-06\n","  6.85607269e-03 1.40058739e-06 1.68639745e-05 1.85425085e-06\n","  9.23928474e-06 2.93161374e-05 1.20621473e-06 1.25753286e-05\n","  1.41234784e-06 2.16936110e-06 1.47416620e-06 1.43794500e-06\n","  8.87849410e-07 3.71453507e-06 7.81526978e-06 1.20053692e-05\n","  9.00859231e-06 1.40769325e-05 2.29171064e-06 9.39419351e-06\n","  3.04799892e-06 3.83067309e-05 1.76510766e-05 7.10661971e-06\n","  3.32655982e-05 4.87843099e-05 8.48974014e-06 4.10736902e-05\n","  5.22139380e-05 9.61601131e-07 1.46240618e-05 2.68716781e-06\n","  2.50119865e-05 1.04382320e-06 3.88984927e-06 4.62439675e-06\n","  6.43332271e-07 1.73218677e-06 1.55485895e-06 1.02707099e-06\n","  1.72871714e-05 1.48620238e-05 3.66397376e-06 2.08305646e-05\n","  6.84995166e-06 3.64095104e-05 2.64041068e-06 9.40142172e-07\n","  2.27256464e-06 1.89522143e-05 1.04016442e-06 1.33164076e-06\n","  2.24304367e-05 4.29976126e-06 1.41335513e-05 4.02941987e-05\n","  7.05960520e-06 4.86066983e-06 1.90000537e-06 1.07121177e-05\n","  1.49903481e-05 6.29010265e-06 5.35092022e-06 5.44181603e-07\n","  2.81687426e-06 3.13745863e-06 2.27684700e-06 3.21977063e-06\n","  2.10775238e-06 9.87972726e-06 7.43238297e-06 5.79448169e-06\n","  3.26854479e-06 9.96530889e-06 4.78034308e-05 2.50537505e-06\n","  7.82323332e-06 6.75425690e-06 7.79897505e-07 1.35844766e-06\n","  2.50051417e-05 1.66461996e-05 4.26891756e-06 2.11424913e-06\n","  2.49113855e-05 4.73188775e-05 5.49745153e-07 5.17739954e-06\n","  1.08316108e-05 1.10197936e-04 5.82917846e-06 1.31792399e-06\n","  1.47417686e-05 2.93954645e-06 2.02223364e-05 1.06295965e-05\n","  3.92138963e-06 8.58933072e-06 2.00639133e-05 2.01753155e-05\n","  9.16307545e-06 2.42078750e-06 3.99180692e-07 2.63372090e-06\n","  2.02376054e-06 6.16143825e-06 5.47308045e-05 2.64988430e-05\n","  4.80232848e-06 5.52535639e-04 6.38559368e-06 3.15991065e-06\n","  9.26264693e-06 3.33396133e-06 5.81859240e-06 1.66468439e-04\n","  6.61154490e-06 1.58618004e-05 1.83536358e-05 9.62224021e-07\n","  8.76957256e-06 1.55593516e-05 9.40000518e-07 5.42972703e-05\n","  9.66374273e-07 6.05353889e-05 1.36064202e-07 3.33720124e-07\n","  4.14688384e-06 3.56294867e-03 4.99707585e-06 1.46048069e-06\n","  3.07005539e-05 1.20726027e-05 5.90855916e-06 3.05859685e-05\n","  2.14215379e-06 2.04090566e-06 4.47271623e-06 4.61015134e-06\n","  3.99229248e-06 7.91140167e-07 3.18798957e-05 1.99046608e-05\n","  4.21036589e-07 1.17123207e-07 1.97408826e-06 5.52496886e-06\n","  3.04690766e-05 1.66424688e-05 6.59914804e-06 3.29855288e-06\n","  1.13360737e-04 2.95788118e-06 8.04884712e-06 3.16231781e-05\n","  5.14586816e-07 8.63826187e-07 5.08385165e-05 2.85553915e-06\n","  1.38320451e-04 2.64299044e-04 4.82061187e-05 8.41175449e-07\n","  9.32853072e-06 2.52729689e-04 3.83877705e-05 1.91131930e-05\n","  6.76077252e-06 2.43994018e-05 3.46321235e-06 8.88927843e-07\n","  2.12027840e-06 3.48005437e-06 5.44286695e-06 3.66266868e-05\n","  1.47926924e-03 1.80217035e-06 3.95978896e-06 2.16835792e-06\n","  5.43034048e-06 6.50678976e-06 1.76680771e-06 1.24926544e-06\n","  5.00945021e-07 3.86280362e-06 2.81369685e-05 1.72387604e-06\n","  1.12096257e-06 8.33513877e-06 2.52714744e-06 5.03218473e-07\n","  6.21008940e-06 7.69252711e-06 2.28259451e-05 4.53861276e-06\n","  1.40575339e-05 1.71083923e-06 1.17417784e-07 3.85564135e-06\n","  2.77339986e-05 6.85088878e-07 3.50320033e-06 3.19821520e-05\n","  2.03885520e-06 6.09560584e-06 2.56652202e-05 1.09588200e-05\n","  5.11878943e-06 3.35238510e-05 2.12092345e-06 3.17784657e-06\n","  8.26319763e-07 8.11082282e-06 2.23530583e-06 3.43926126e-06\n","  2.47823959e-06 1.64698486e-05 1.03276773e-06 3.30147054e-06\n","  2.25111307e-06 1.44940750e-05 1.84633773e-05 5.18334764e-06\n","  5.13616305e-06 2.54383167e-06 1.51965714e-05 3.23595668e-06\n","  9.70775363e-06 9.18439127e-07 1.22474614e-06 8.02556133e-06\n","  9.84557573e-06 9.08197876e-07 2.04673288e-05 2.52879290e-05\n","  9.63818002e-06 1.59413703e-05 1.73112397e-07 5.62906826e-06\n","  5.15359861e-05 5.70257453e-06 1.96295114e-05 7.33874185e-06\n","  7.12864335e-07 3.99994678e-05 1.32513924e-06 1.62167260e-06\n","  5.97209873e-06 1.21071048e-06 4.36720427e-07 4.69708766e-07\n","  1.78031848e-06 1.34108777e-04 2.26131624e-06 3.41014788e-06\n","  2.48055676e-06 1.65047311e-06 1.80763016e-06 1.79679105e-07\n","  4.99883481e-06 1.92713424e-06 3.49888001e-06 5.82648863e-06\n","  5.18299885e-07 9.30407509e-07 9.67314804e-07 2.24006476e-06\n","  2.59933586e-06 2.67438736e-06 8.16383761e-07 2.89797185e-06\n","  2.53408120e-06 4.94339156e-06 2.14454758e-06 9.05922479e-06\n","  3.26929285e-06 3.81083100e-06 1.19765746e-05 8.33993681e-07\n","  6.32223248e-07 2.85992701e-06 1.92856805e-06 4.51379265e-06\n","  1.59547767e-06 8.29069108e-07 2.60565230e-06 1.30128467e-06\n","  3.85151992e-07 2.94946449e-06 1.54607369e-05 2.62028243e-06\n","  2.60282013e-05 4.38492634e-06 8.27205622e-06 3.22326850e-05\n","  4.95900122e-06 6.88518355e-07 1.40101224e-06 1.52228330e-03\n","  8.75077092e-07 1.17028096e-04 1.44954984e-05 3.23946733e-06\n","  2.24120669e-07 4.28341082e-06 1.94343393e-05 2.24314726e-06\n","  3.37746837e-06 6.23839742e-06 9.92096858e-08 2.15435193e-06\n","  1.53305700e-06 1.15896437e-05 1.00759044e-05 1.21113555e-06\n","  5.80865162e-06 2.54470137e-07 2.82898372e-06 4.10603570e-06\n","  2.46962202e-07 2.83141674e-07 1.25072393e-05 9.64455758e-05]]\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.applications.resnet50 import decode_predictions\n","decode_predictions(pred, top=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"46sTfxZsS0Gv","executionInfo":{"status":"ok","timestamp":1716985265185,"user_tz":-540,"elapsed":382,"user":{"displayName":"허진경 (나자바바)","userId":"08806215878196109838"}},"outputId":"d1d856d6-5648-4a1e-ba9d-3ecd1c9df8c9"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n","35363/35363 [==============================] - 0s 0us/step\n"]},{"output_type":"execute_result","data":{"text/plain":["[[('n02099712', 'Labrador_retriever', 0.26897743),\n","  ('n02108089', 'boxer', 0.15149544),\n","  ('n02099849', 'Chesapeake_Bay_retriever', 0.10756537),\n","  ('n02089867', 'Walker_hound', 0.067649655),\n","  ('n02104029', 'kuvasz', 0.046868663)]]"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["from tensorflow.keras.applications import ResNet50\n","cnn_model = ResNet50(input_shape=(224,224,3), include_top=False)\n","cnn_model.trainable = True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VvOYNJOcTid4","executionInfo":{"status":"ok","timestamp":1716985598516,"user_tz":-540,"elapsed":4609,"user":{"displayName":"허진경 (나자바바)","userId":"08806215878196109838"}},"outputId":"62c18a8f-2ebc-4f2f-a003-54d0f298d466"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n","94765736/94765736 [==============================] - 1s 0us/step\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Flatten, Dense"],"metadata":{"id":"J4BJEEyEUP5J","executionInfo":{"status":"ok","timestamp":1716985746675,"user_tz":-540,"elapsed":310,"user":{"displayName":"허진경 (나자바바)","userId":"08806215878196109838"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["model = Sequential()\n","model.add(cnn_model)\n","model.add(Flatten())\n","model.add(Dense(1024, activation='relu'))\n","model.add(Dense(100, activation='softmax')) # 분류 클래스 라벨 개수는 100개\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1qdoILyVUzuo","executionInfo":{"status":"ok","timestamp":1716985856898,"user_tz":-540,"elapsed":2887,"user":{"displayName":"허진경 (나자바바)","userId":"08806215878196109838"}},"outputId":"534fd1d5-97e3-4459-c07d-7e75c26054d7"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n","                                                                 \n"," flatten_1 (Flatten)         (None, 100352)            0         \n","                                                                 \n"," dense_1 (Dense)             (None, 1024)              102761472 \n","                                                                 \n"," dense_2 (Dense)             (None, 100)               102500    \n","                                                                 \n","=================================================================\n","Total params: 126451684 (482.37 MB)\n","Trainable params: 126398564 (482.17 MB)\n","Non-trainable params: 53120 (207.50 KB)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ejNfvigiV6Bx"},"execution_count":null,"outputs":[]}]}